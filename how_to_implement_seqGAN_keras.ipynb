{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!/usr/bin/env python3\r\n",
    "# -*- coding: utf-8 -*-\r\n",
    "\"\"\"\r\n",
    "Created on Thu Oct 12 12:04:31 2017\r\n",
    "\r\n",
    "@author: gama\r\n",
    "\"\"\"\r\n",
    "from  keras.utils import multi_gpu_model\r\n",
    "import tensorflow as tf\r\n",
    "import re\r\n",
    "import numpy as np\r\n",
    "from keras.models import Model\r\n",
    "from keras.layers import Embedding,Masking\r\n",
    "from keras.layers import Input, Dense,Reshape,concatenate,Flatten,Activation,Permute,multiply\r\n",
    "from keras.layers import GRU,Conv1D,GlobalMaxPooling1D,TimeDistributed,RepeatVector,LSTM,MaxPooling1D\r\n",
    "from keras import optimizers\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from keras.layers import Lambda,Dropout\r\n",
    "from keras.utils import to_categorical,multi_gpu_model\r\n",
    "import gc\r\n",
    "import random\r\n",
    "import nltk\r\n",
    "import math\r\n",
    "from tqdm import tqdm,tqdm_notebook\r\n",
    "import csv\r\n",
    "import keras.backend as K\r\n",
    "from keras.models import load_model\r\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_trainable(net, val):\r\n",
    "    net.trainable = val\r\n",
    "    for l in net.layers:\r\n",
    "        l.trainable = val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ppx(y_true, y_pred):\r\n",
    "     loss = K.sparse_categorical_crossentropy(y_true, y_pred)\r\n",
    "     perplexity = K.cast(K.pow(math.e, K.mean(loss, axis=-1)), K.floatx())\r\n",
    "     return perplexity\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model(\r\n",
    "    que_pad,\r\n",
    "    num_word,\r\n",
    "):\r\n",
    "    dim = 256\r\n",
    "    inputs = Input(\r\n",
    "        shape=(que_pad,)\r\n",
    "    )\r\n",
    "\r\n",
    "    g_emb = Embedding(\r\n",
    "        num_words + 1,\r\n",
    "        dim,\r\n",
    "        mask_zero=True,\r\n",
    "        input_length=(que_pad)\r\n",
    "    )(inputs)\r\n",
    "\r\n",
    "    decoder = GRU(\r\n",
    "        dim\r\n",
    "    )(g_emb)\r\n",
    "    decoder = Dense(\r\n",
    "        num_words,\r\n",
    "        activation=\"softmax\"\r\n",
    "    )(decoder)\r\n",
    "    \r\n",
    "    model = Model(\r\n",
    "        inputs=inputs,\r\n",
    "        outputs=decoder\r\n",
    "    )\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def output_sequence(\r\n",
    "    pair_train1,\r\n",
    "    pair_train2,\r\n",
    "    num,\r\n",
    "    g_model,\r\n",
    "    ans_pad\r\n",
    "):\r\n",
    "    word2 = []\r\n",
    "    test = [pair_train1[num]]\r\n",
    "    # print(test)\r\n",
    "    test = np.array(test)\r\n",
    "    index = g_model.predict(test)\r\n",
    "    index = np.argmax(index[0], axis=-1)      \r\n",
    "    word2.append(index)\r\n",
    "    for i in range(ans_pad - 1):\r\n",
    "        test = np.delete(test, ans_pad, 1)\r\n",
    "        test = np.concatenate([test, [[index]]], axis=1)\r\n",
    "        index = g_model.predict(test)\r\n",
    "        index = np.argmax(index[0], axis=-1)\r\n",
    "        word2.append(index)\r\n",
    "        if str(index) == str(word2idx['EOS']):\r\n",
    "              break\r\n",
    "    que = []\r\n",
    "    sample = []\r\n",
    "    test = [pair_train1[num] + pair_train2[num]]\r\n",
    "    for g in test[0]:\r\n",
    "          for value, age in word2idx.items():\r\n",
    "                if age == g:\r\n",
    "                \tque.append(value)\r\n",
    "    for g in word2:\r\n",
    "          for value, age in word2idx.items():\r\n",
    "                if age == g:\r\n",
    "                \tsample.append(value)\r\n",
    "    print('question')\r\n",
    "    print(''.join(que))\r\n",
    "    print('RAP_model') \r\n",
    "    print(''.join(sample))\r\n",
    "    \r\n",
    "    que = que[0:20] + ['   ans:   '] + que[ans_pad:]      \r\n",
    "    return  ''.join(que),''.join(sample)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def distribution_index(fake):\r\n",
    "    fake_idx = []\r\n",
    "    \r\n",
    "    for i in fake:\r\n",
    "    \tw = np.argmax(i)\r\n",
    "    \tfake_idx.append([w])\r\n",
    "    fake_idx = np.array(fake_idx)\r\n",
    "\r\n",
    "    return fake_idx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rollout_reward(\r\n",
    "    train_x,\r\n",
    "    train_y,\r\n",
    "    count,\r\n",
    "    candidate,\r\n",
    "    ans_pad\r\n",
    "):\r\n",
    "    rollout_sample = []\r\n",
    "    new_input = np.array(train_x)\r\n",
    "    # print(new_input.shape)\r\n",
    "    new_x = []\r\n",
    "    new_y = []\r\n",
    "    new_input_x = []\r\n",
    "    new_input_y = []\r\n",
    "    reward = []\r\n",
    "    \r\n",
    "    for i in range(count):\r\n",
    "        fake = g_model.predict(new_input)\r\n",
    "        old_input = new_input\r\n",
    "        new_input = []\r\n",
    "        for k in range(len(fake)):\r\n",
    "            # print(k)\r\n",
    "            index = np.argsort(fake[k])\r\n",
    "            index = index[::-1] # 排序由機率大到小\r\n",
    "            for key, j in enumerate(index[0:candidate]):\r\n",
    "                if i == 0:\r\n",
    "                    new_input_x.append(train_x[0])\r\n",
    "                    new_input_y.append([j])\r\n",
    "                new_x.append(new_input)\r\n",
    "                new_y.append([j])\r\n",
    "                con = np.delete(old_input[k], ans_pad, 0)\r\n",
    "                con = np.concatenate([con, [j]], axis=0)\r\n",
    "                new_input.append(con)\r\n",
    "                \r\n",
    "        new_input = np.array(new_input)\r\n",
    "        \r\n",
    "    new_y = np.array(new_y)\r\n",
    "    # print(new_input,new_y) \r\n",
    "    k = len(new_input) // candidate\r\n",
    "    for i in range(candidate):\r\n",
    "        vector = discriminator.predict(\r\n",
    "            [new_input[k * i:k * (i + 1)],\r\n",
    "             new_y[k * i:k * (i + 1)]   ]\r\n",
    "        )[:,0]\r\n",
    "        reward.append(np.mean(vector))\r\n",
    "#     reward.append(discriminator.predict([train_x, train_y])[0][0])\r\n",
    "    # print(reward)\r\n",
    "    reward=reward-np.mean(reward)\r\n",
    "#     new_input_x.append(train_x[0])\r\n",
    "#     new_input_y.append(train_y[0])\r\n",
    "    return new_input_x, new_input_y, reward.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def greedy_search(\r\n",
    "    g_model,\r\n",
    "    train_x,\r\n",
    "    train_y,\r\n",
    "    ans_pad\r\n",
    "):\r\n",
    "    new_trainX = []\r\n",
    "    new_trainY = []\r\n",
    "\r\n",
    "    for key, i in enumerate(tqdm_notebook(train_x)):\r\n",
    "        if key % ans_pad == 0:\r\n",
    "            train_w = i\r\n",
    "        else:\r\n",
    "            #train_w[ans_pad+key%ans_pad-1]=index\r\n",
    "            train_w = np.delete(train_w, ans_pad, 0)   \r\n",
    "            train_w = np.concatenate([train_w, [index]], axis=0)    \r\n",
    "            \r\n",
    "        index = g_model.predict(np.array([train_w]))\r\n",
    "        index = np.argmax(index[0], axis=-1)\r\n",
    "        new_trainX.append(train_w)    \r\n",
    "        new_trainY.append([index])\r\n",
    "\r\n",
    "    new_trainX = np.array(new_trainX)\r\n",
    "    new_trainY = np.array(new_trainY)\r\n",
    "\r\n",
    "    return new_trainX, new_trainY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_d(discriminator, train_x, train_y, X, Y):\r\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=1, verbose=2, mode='auto')\r\n",
    "    n = len(train_x)\r\n",
    "    YT = np.zeros([n * 2, 2])\r\n",
    "    YT[0:n, 1] = 1\r\n",
    "    YT[n:, 0] = 1\r\n",
    "    # fake = distribution_index(fake)\r\n",
    "    XT = np.concatenate([X, train_x])\r\n",
    "    XT2 = np.concatenate([Y, train_y])\r\n",
    "    \r\n",
    "    result = discriminator.fit(\r\n",
    "        [XT, XT2],\r\n",
    "        YT,\r\n",
    "        epochs=1,\r\n",
    "        shuffle=True,\r\n",
    "        batch_size=32,\r\n",
    "        verbose=0,\r\n",
    "        callbacks=[earlyStopping]\r\n",
    "    )\r\n",
    "\r\n",
    "    return result.history['acc'][-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def return_all_mean(train_x, train_y):\r\n",
    "    fake = g_model.predict(train_x)\r\n",
    "    # per = s_model.evaluate(train_x, train_y, batch_size=512)\r\n",
    "\r\n",
    "    #per=1/per[0]\r\n",
    "    mean = discriminator.predict([train_x, distribution_index(fake)])\r\n",
    "    return np.mean(mean[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#setting tf_config\r\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\r\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True))\r\n",
    "K.set_session(sess)\r\n",
    "# jieba.load_userdict('dict.txt.big.txt')\r\n",
    "# jieba.load_userdict('NameDict_Ch_v2.txt')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Load Dataset\r\n",
    "post = []\r\n",
    "response = []\r\n",
    "\r\n",
    "post_file = open('QA dataset/stc_weibo_train_post','r')\r\n",
    "for i in post_file.readlines():\r\n",
    "    post.append(i.split())\r\n",
    "\r\n",
    "post_file = open('QA dataset/stc_weibo_train_response','r')\r\n",
    "for i in post_file.readlines():\r\n",
    "    response.append(i.split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "token_stream = []\r\n",
    "que_pad = 20\r\n",
    "ans_pad = 10\r\n",
    "stop_count = 0\r\n",
    "\r\n",
    "pair_train1 = []\r\n",
    "pair_train2 = []\r\n",
    "check_stop = []\r\n",
    "\r\n",
    "count = 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for key, i in enumerate(post):\r\n",
    "    if len(pair_train1) >= 100000:\r\n",
    "        break\r\n",
    "    if len(i) >= ans_pad or len(response[key]) >= ans_pad or len(i) < 3 or len(response[key]) < 3:\r\n",
    "        continue\r\n",
    "\r\n",
    "    while len(i) < que_pad:\r\n",
    "        i.append('PAD')\r\n",
    "\r\n",
    "    while len(response[key]) < ans_pad:\r\n",
    "        response[key].append('PAD')\r\n",
    "\r\n",
    "    token_stream.extend(i)\r\n",
    "    pair_train1.append(i)    \r\n",
    "    token_stream.extend(response[key])\r\n",
    "    pair_train2.append(response[key])    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('# of pairs: ', len(pair_train1))\r\n",
    "pair = len(pair_train1)\r\n",
    "\r\n",
    "\r\n",
    "words = list(set(token_stream))\r\n",
    "words.remove('PAD')\r\n",
    "\r\n",
    "word2idx = {}\r\n",
    "word2idx['PAD'] = 0\r\n",
    "for i, word in enumerate(words):\r\n",
    "    word2idx[word] = i + 1\r\n",
    "num_words = len(word2idx)\r\n",
    "print(\"# of words: \", num_words)\r\n",
    "                    \r\n",
    "print('process_data')\r\n",
    "\r\n",
    "predict_pair = ans_pad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(pair_train1)):\r\n",
    "    for j in range(que_pad):\r\n",
    "        pair_train1[i][j] = word2idx[pair_train1[i][j]]\r\n",
    "\r\n",
    "for i in range(len(pair_train2)):\r\n",
    "    for j in range(ans_pad):\r\n",
    "        pair_train2[i][j] = word2idx[pair_train2[i][j]]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x = []\r\n",
    "train_y = []\r\n",
    "pad_sequence = [word2idx['PAD']] * ans_pad\r\n",
    "for i in range(len(pair_train1)):\r\n",
    "    for j in range(ans_pad):\r\n",
    "        forward = pair_train1[i][:ans_pad]\r\n",
    "        backward = pad_sequence[j:ans_pad]\r\n",
    "        #print(backward)\r\n",
    "        train_x.append(forward + backward + pair_train2[i][:j]) \r\n",
    "        train_y.append([pair_train2[i][j]])\r\n",
    "\r\n",
    " \r\n",
    "train_x = np.array(train_x)\r\n",
    "train_y = np.array(train_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_x.shape)\r\n",
    "print(train_y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g_model = build_model(\r\n",
    "    que_pad,\r\n",
    "    num_words\r\n",
    ")\r\n",
    "#sampling_model= multi_gpu_model(get_model(), gpus=2)\r\n",
    "g_model.compile(loss=ppx, optimizer='adam',metrics=['accuracy'])\r\n",
    "earlyStopping = EarlyStopping(monitor='loss', patience=2, verbose=2, mode='auto')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g_model.fit(\r\n",
    "    train_x,\r\n",
    "    train_y,\r\n",
    "    epochs=49,\r\n",
    "    batch_size=512,\r\n",
    "    validation_split=0.2,\r\n",
    "    verbose=1,\r\n",
    "    callbacks=[earlyStopping]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s_model = g_model # prepare assignment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "update_g = len(pair_train1)\r\n",
    "for i in range(5):\r\n",
    "    output_sequence(\r\n",
    "        pair_train1,\r\n",
    "        pair_train2,\r\n",
    "        random.randint(0, random.randint(0, pair-1)),\r\n",
    "        g_model\r\n",
    "    )\r\n",
    "\r\n",
    "old_result = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dim = 128\r\n",
    "d_input1 = Input(shape=(que_pad,))\r\n",
    "d_input2 = Input(shape=(1,))\r\n",
    "#,mask_zero = True\r\n",
    "con = concatenate([d_input1, d_input2], axis=1)\r\n",
    "d_emb = Embedding(num_words + 1, dim, input_length=(que_pad + 1))(con)\r\n",
    "sent_representation = LSTM(dim)(d_emb)\r\n",
    "#sent_representation = GlobalMaxPooling1D()(cnn)\r\n",
    "probabilities = Dense(2, activation='softmax')(sent_representation)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "discriminator = Model(\r\n",
    "    [d_input1, d_input2],\r\n",
    "    probabilities\r\n",
    ")\r\n",
    "\r\n",
    "discriminator.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss=\"categorical_crossentropy\", \r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epoch = 0\r\n",
    "# for s in range(120):\r\n",
    "for x in range(10):\r\n",
    "#     print(\"greedy-search\")\r\n",
    "#     X, Y = greedy_search(g_model, train_x[::ans_pad], train_y[::ans_pad])\r\n",
    "#     print('d-step')\r\n",
    "#     result = train_d(discriminator, train_x[::ans_pad], train_y[::ans_pad], X, Y)\r\n",
    "#     del X, Y\r\n",
    "#     print(result)\r\n",
    "\r\n",
    "#     print('finishsearch')\r\n",
    "    count = 0   \r\n",
    "    dis_x = []\r\n",
    "    dis_y = []\r\n",
    "    reward = []\r\n",
    "    for g in tqdm_notebook(range(int(len(train_x)))):\r\n",
    "        if g % ans_pad == 0:\r\n",
    "             code = 0\r\n",
    "        new_trainX = []\r\n",
    "        new_trainY = []\r\n",
    "        if g % ans_pad == 0:\r\n",
    "            train_w = train_x[g]    \r\n",
    "        index = g_model.predict(np.array([train_w]))\r\n",
    "        index = np.argmax(index[0], axis=-1)\r\n",
    "        new_trainX.append(train_w)    \r\n",
    "        new_trainY.append([index])\r\n",
    "        if g % ans_pad != 0:\r\n",
    "            train_w = np.delete(train_w, ans_pad, 0)   \r\n",
    "            train_w = np.concatenate([train_w, [index]], axis=0)\r\n",
    "        s = rollout_reward(new_trainX, new_trainY, (ans_pad - g % ans_pad), 2, ans_pad)    \r\n",
    "        dis_x.extend(s[0])\r\n",
    "        dis_y.extend(s[1]) \r\n",
    "        reward.extend(s[2])\r\n",
    "        code += 1 \r\n",
    "        if g % 10000 == 0 and g != 0:\r\n",
    "            print('batch', g)\r\n",
    "            g_model.fit(\r\n",
    "                np.array(dis_x),\r\n",
    "                np.array(dis_y),\r\n",
    "                epochs=1,\r\n",
    "                batch_size=256,\r\n",
    "                sample_weight=np.array(reward),\r\n",
    "                verbose=0\r\n",
    "            )\r\n",
    "            dis_x=[]\r\n",
    "            dis_y=[]\r\n",
    "            reward=[]\r\n",
    "\r\n",
    "            for i in range(3):\r\n",
    "                output_sequence(\r\n",
    "                    pair_train1,\r\n",
    "                    pair_train2,\r\n",
    "                    random.randint(0, random.randint(0, pair-1)),\r\n",
    "                    g_model\r\n",
    "                )\r\n",
    "                \r\n",
    "    epoch = epoch + 1\r\n",
    "    print('epoch', epoch) \r\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(3):\r\n",
    "    output_sequence(\r\n",
    "        pair_train1,\r\n",
    "        pair_train2,\r\n",
    "        random.randint(0, random.randint(0, pair-1)),\r\n",
    "        g_model\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tensorflow-1.15': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "5169187fa7ffabf2bc47513fc02451fc0c08951b1e9bd20bce5d1b349a195c2a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}